{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naver 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://nid.naver.com/nidlogin.login'\n",
    "id_ = ''\n",
    "pw = ''\n",
    "    \n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)\n",
    "\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "# Naver login 네이버 로그인\n",
    "browser.execute_script(\"document.getElementsByName('id')[0].value=\\'\"+ id_ + \"\\'\")\n",
    "browser.execute_script(\"document.getElementsByName('pw')[0].value=\\'\"+ pw + \"\\'\")\n",
    "browser.find_element(by=By.XPATH,value='//*[@id=\"log.login\"]').click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 키워드 하나에 대해서 csv파일 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 카페에 들어가 원하는 키워드 입력 후 게시글의 링크와 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지에서 게시글 50개 수집 완료. 총 수집: 50개\n",
      "2 페이지에서 게시글 50개 수집 완료. 총 수집: 100개\n",
      "3 페이지에서 게시글 50개 수집 완료. 총 수집: 150개\n",
      "4 페이지에서 게시글 50개 수집 완료. 총 수집: 200개\n",
      "5 페이지에서 게시글 50개 수집 완료. 총 수집: 250개\n",
      "6 페이지에서 게시글 50개 수집 완료. 총 수집: 300개\n",
      "7 페이지에서 게시글 50개 수집 완료. 총 수집: 350개\n",
      "8 페이지에서 게시글 44개 수집 완료. 총 수집: 394개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "총 수집된 게시글 수: 394\n"
     ]
    }
   ],
   "source": [
    "baseurl='https://cafe.naver.com/directwedding'\n",
    "browser.get(baseurl)\n",
    "\n",
    "keyword = '\"lg\" \"스타일러\"'\n",
    "search_box = browser.find_element(By.ID, \"topLayerQueryInput\")\n",
    "search_box.send_keys(keyword)\n",
    "search_box.send_keys(Keys.ENTER)\n",
    "time.sleep(1)\n",
    "browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "# \"50개씩\" 보기 옵션을 선택하기 위한 드롭다운 메뉴 클릭\n",
    "dropdown_menu = browser.find_element(By.ID, \"listSizeSelectDiv\")\n",
    "dropdown_menu.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# \"50개씩\" 옵션 선택\n",
    "fifty_option = browser.find_element(\n",
    "    By.XPATH, \"//a[contains(text(), '50개씩')]\")\n",
    "fifty_option.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 한번에 보이는 게시물수를 변경하면 \"lg스타일러\"가 /\"lg스타일러/\"로 변함\n",
    "# 따라서 다시 검색..\n",
    "search_box = browser.find_element(By.ID, \"queryTop\")\n",
    "search_box.clear()\n",
    "search_box.send_keys(keyword)\n",
    "time.sleep(1)\n",
    "\n",
    "# 후기(가전) 게시판 선택\n",
    "board = browser.find_element(By.ID, \"divSearchMenuTop\")\n",
    "board.click()\n",
    "time.sleep(1)\n",
    "\n",
    "board.option = browser.find_element(\n",
    "    By.XPATH, \"//a[contains(text(), '후기(가전)')]\")\n",
    "board.option.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 검색 옵션 드롭다운 메뉴 클릭\n",
    "search_option_dropdown = browser.find_element(By.ID, \"divSearchByTop\")\n",
    "search_option_dropdown.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# '제목만' 옵션 선택\n",
    "title_only_option = browser.find_element(\n",
    "    By.XPATH, \"//a[contains(text(), '제목만')]\")\n",
    "title_only_option.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 검색 버튼 클릭\n",
    "search_btn = browser.find_element(\n",
    "    By.XPATH, \"//button[contains(text(), '검색')]\")\n",
    "search_btn.click()\n",
    "time.sleep(1)\n",
    "\n",
    "titles = []\n",
    "links = []\n",
    "\n",
    "while True:\n",
    "    # BeautifulSoup으로 HTML을 파싱\n",
    "    soup = bs(browser.page_source, 'html.parser')\n",
    "    articles = soup.select('div.inner_list a.article')\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.text.strip()\n",
    "        link = article['href']\n",
    "        titles.append(title)\n",
    "        links.append(link)\n",
    "\n",
    "    page_num = browser.find_element(By.CSS_SELECTOR, \"a.on\").text\n",
    "    print(f\"{page_num} 페이지에서 게시글 {len(articles)}개 수집 완료. 총 수집: {len(titles)}개\")\n",
    "\n",
    "    # 다음 페이지 버튼 클릭\n",
    "    try:\n",
    "        # 현재 페이지 버튼 찾기\n",
    "        current_page = browser.find_element(By.CSS_SELECTOR, \"a.on\")\n",
    "        next_page = current_page.find_element(By.XPATH, \"following-sibling::a\")\n",
    "        next_page.click()\n",
    "        time.sleep(2)\n",
    "    except Exception:\n",
    "        print(\"더 이상 이동할 페이지가 없습니다. 크롤링 종료.\")\n",
    "        break\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"총 수집된 게시글 수: {len(titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 링크 따라서 본문 크롤링 후 csv파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for title, link in zip(titles, links):\n",
    "    retries = 1 # 해당 글을 몇번 들어갈건지 선정\n",
    "    success = False # 글 접속 성공여부\n",
    "\n",
    "    while retries > 0 and not success:\n",
    "        # 게시글의 링크로 이동\n",
    "        browser.get('https://cafe.naver.com/directwedding' + link)\n",
    "        browser.switch_to.frame(\"cafe_main\")\n",
    "        page_source = browser.page_source\n",
    "        soup_article = bs(page_source, 'html.parser')\n",
    "        # Date\n",
    "        date = soup_article.find('div', class_='tit-box').find('td', class_='m-tcol-c date').text.strip()\n",
    "        # Nickname\n",
    "        nickname = soup_article.find('td', class_='p-nick').text.strip()\n",
    "        # content\n",
    "        lines = soup_article.find('div', class_='tbody m-tcol-c').stripped_strings\n",
    "        content = ' '.join(lines)\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        content = content.replace(pattern2, '')\n",
    "        content = content.replace('\\n', '')\n",
    "        content = content.replace('\\u200b', '')\n",
    "\n",
    "        retries -= 1  # 재시도 횟수 감소\n",
    "\n",
    "    results.append({\n",
    "        'Keyword': keyword,\n",
    "        'Title': title,\n",
    "        'Date': date,\n",
    "        'Link': 'https://cafe.naver.com/directwedding' + link,\n",
    "        'Content': content\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data=results)\n",
    "df.to_csv(\"lg스타일러.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 키워드 검색결과를 하나의 csv파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"무선스틱청소기\" 크롤링 시작\n",
      "1 페이지에서 게시글 6개 수집 완료. 총 수집: 6개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "\"무선스틱청소기\" 총 수집된 게시글 수: 6\n",
      "\n",
      "\"무선청소기\" 크롤링 시작\n",
      "1 페이지에서 게시글 50개 수집 완료. 총 수집: 50개\n",
      "2 페이지에서 게시글 50개 수집 완료. 총 수집: 100개\n",
      "3 페이지에서 게시글 50개 수집 완료. 총 수집: 150개\n",
      "4 페이지에서 게시글 50개 수집 완료. 총 수집: 200개\n",
      "5 페이지에서 게시글 50개 수집 완료. 총 수집: 250개\n",
      "6 페이지에서 게시글 50개 수집 완료. 총 수집: 300개\n",
      "7 페이지에서 게시글 50개 수집 완료. 총 수집: 350개\n",
      "8 페이지에서 게시글 50개 수집 완료. 총 수집: 400개\n",
      "9 페이지에서 게시글 49개 수집 완료. 총 수집: 449개\n",
      "10 페이지에서 게시글 50개 수집 완료. 총 수집: 499개\n",
      "11 페이지에서 게시글 50개 수집 완료. 총 수집: 549개\n",
      "12 페이지에서 게시글 50개 수집 완료. 총 수집: 599개\n",
      "13 페이지에서 게시글 32개 수집 완료. 총 수집: 631개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "\"무선청소기\" 총 수집된 게시글 수: 631\n",
      "\n",
      "\"lg\" \"무선청소기\" 크롤링 시작\n",
      "1 페이지에서 게시글 50개 수집 완료. 총 수집: 50개\n",
      "2 페이지에서 게시글 50개 수집 완료. 총 수집: 100개\n",
      "3 페이지에서 게시글 41개 수집 완료. 총 수집: 141개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "\"lg\" \"무선청소기\" 총 수집된 게시글 수: 141\n",
      "\n",
      "\"삼성\" \"무선청소기\" 크롤링 시작\n",
      "1 페이지에서 게시글 50개 수집 완료. 총 수집: 50개\n",
      "2 페이지에서 게시글 50개 수집 완료. 총 수집: 100개\n",
      "3 페이지에서 게시글 24개 수집 완료. 총 수집: 124개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "\"삼성\" \"무선청소기\" 총 수집된 게시글 수: 124\n",
      "\n",
      "\"다이슨\" \"무선청소기\" 크롤링 시작\n",
      "1 페이지에서 게시글 32개 수집 완료. 총 수집: 32개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "\"다이슨\" \"무선청소기\" 총 수집된 게시글 수: 32\n",
      "\n",
      "\"로봇청소기\" 크롤링 시작\n",
      "1 페이지에서 게시글 50개 수집 완료. 총 수집: 50개\n",
      "2 페이지에서 게시글 50개 수집 완료. 총 수집: 100개\n",
      "3 페이지에서 게시글 50개 수집 완료. 총 수집: 150개\n",
      "4 페이지에서 게시글 50개 수집 완료. 총 수집: 200개\n",
      "5 페이지에서 게시글 50개 수집 완료. 총 수집: 250개\n",
      "6 페이지에서 게시글 50개 수집 완료. 총 수집: 300개\n",
      "7 페이지에서 게시글 50개 수집 완료. 총 수집: 350개\n",
      "8 페이지에서 게시글 50개 수집 완료. 총 수집: 400개\n",
      "9 페이지에서 게시글 50개 수집 완료. 총 수집: 450개\n",
      "10 페이지에서 게시글 50개 수집 완료. 총 수집: 500개\n",
      "11 페이지에서 게시글 50개 수집 완료. 총 수집: 550개\n",
      "12 페이지에서 게시글 50개 수집 완료. 총 수집: 600개\n",
      "13 페이지에서 게시글 50개 수집 완료. 총 수집: 650개\n",
      "14 페이지에서 게시글 50개 수집 완료. 총 수집: 700개\n",
      "15 페이지에서 게시글 50개 수집 완료. 총 수집: 750개\n",
      "16 페이지에서 게시글 50개 수집 완료. 총 수집: 800개\n",
      "17 페이지에서 게시글 49개 수집 완료. 총 수집: 849개\n",
      "18 페이지에서 게시글 50개 수집 완료. 총 수집: 899개\n",
      "19 페이지에서 게시글 49개 수집 완료. 총 수집: 948개\n",
      "20 페이지에서 게시글 50개 수집 완료. 총 수집: 998개\n",
      "21 페이지에서 게시글 13개 수집 완료. 총 수집: 1011개\n",
      "더 이상 이동할 페이지가 없습니다. 크롤링 종료.\n",
      "\"로봇청소기\" 총 수집된 게시글 수: 1011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wanted naver cafe url\n",
    "baseurl='https://cafe.naver.com/directwedding'\n",
    "browser.get(baseurl)\n",
    "\n",
    "# wanted keyword list\n",
    "Search_Keyword = ['\"무선스틱청소기\"', '\"무선청소기\"', '\"lg\" \"무선청소기\"', '\"삼성\" \"무선청소기\"', '\"다이슨\" \"무선청소기\"', '\"로봇청소기\"']\n",
    "\n",
    "results = []\n",
    "keyword_list = []\n",
    "titles_list = []\n",
    "links_list = []\n",
    "\n",
    "for keyword in Search_Keyword:\n",
    "    search_box = browser.find_element(By.ID, \"topLayerQueryInput\")\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(1)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "    # \"50개씩\" 보기 옵션을 선택하기 위한 드롭다운 메뉴 클릭\n",
    "    dropdown_menu = browser.find_element(By.ID, \"listSizeSelectDiv\")\n",
    "    dropdown_menu.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # \"50개씩\" 옵션 선택\n",
    "    fifty_option = browser.find_element(By.XPATH, \"//a[contains(text(), '50개씩')]\")\n",
    "    fifty_option.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 한번에 보이는 게시물수를 변경하면 \"lg스타일러\"가 /\"lg스타일러/\"로 변함\n",
    "    # 따라서 다시 검색..\n",
    "    search_box = browser.find_element(By.ID, \"queryTop\")\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(keyword)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 후기(가전) 게시판 선택\n",
    "    board = browser.find_element(By.ID, \"divSearchMenuTop\")\n",
    "    board.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    board.option = browser.find_element(By.XPATH, \"//a[contains(text(), '후기(가전)')]\")\n",
    "    board.option.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 검색 옵션 드롭다운 메뉴 클릭\n",
    "    search_option_dropdown = browser.find_element(By.ID, \"divSearchByTop\")\n",
    "    search_option_dropdown.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # '제목만' 옵션 선택\n",
    "    title_only_option = browser.find_element(By.XPATH, \"//a[contains(text(), '제목만')]\")\n",
    "    title_only_option.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 검색 버튼 클릭\n",
    "    search_btn = browser.find_element(By.XPATH, \"//button[contains(text(), '검색')]\")\n",
    "    search_btn.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    titles = []\n",
    "    links = []\n",
    "\n",
    "    print(f'{keyword} 크롤링 시작')\n",
    "\n",
    "    while True:\n",
    "        # BeautifulSoup으로 HTML을 파싱\n",
    "        soup = bs(browser.page_source, 'html.parser')\n",
    "        articles = soup.select('div.inner_list a.article')\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.text.strip()\n",
    "            link = article['href']\n",
    "            keyword_list.append(keyword)\n",
    "            titles.append(title)\n",
    "            links.append(link)\n",
    "\n",
    "        page_num = browser.find_element(By.CSS_SELECTOR, \"a.on\").text\n",
    "        print(f\"{page_num} 페이지에서 게시글 {len(articles)}개 수집 완료. 총 수집: {len(titles)}개\")\n",
    "\n",
    "        # 다음 페이지 버튼 클릭\n",
    "        try:\n",
    "            # 현재 페이지 버튼 찾기\n",
    "            current_page = browser.find_element(By.CSS_SELECTOR, \"a.on\")\n",
    "            next_page = current_page.find_element(By.XPATH, \"following-sibling::a\")\n",
    "            next_page.click()\n",
    "            time.sleep(2)\n",
    "        except Exception:\n",
    "            print(\"더 이상 이동할 페이지가 없습니다. 크롤링 종료.\")\n",
    "            break\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"{keyword} 총 수집된 게시글 수: {len(titles)}\\n\")\n",
    "    titles_list.extend(titles)\n",
    "    links_list.extend(links)\n",
    "\n",
    "    browser.get(baseurl)\n",
    "\n",
    "# 본문 크롤링\n",
    "for keyword, title, link in zip(keyword_list, titles_list, links_list):\n",
    "    retries = 1 # 해당 글을 몇번 들어갈건지 선정\n",
    "    success = False # 글 접속 성공여부\n",
    "\n",
    "    while retries > 0 and not success:\n",
    "        # 게시글의 링크로 이동\n",
    "        browser.get('https://cafe.naver.com/directwedding' + link)\n",
    "        browser.switch_to.frame(\"cafe_main\")\n",
    "        page_source = browser.page_source\n",
    "        soup_article = bs(page_source, 'html.parser')\n",
    "        # Date\n",
    "        date = soup_article.find('div', class_='tit-box').find('td', class_='m-tcol-c date').text.strip()\n",
    "        # Nickname\n",
    "        nickname = soup_article.find('td', class_='p-nick').text.strip()\n",
    "        # content\n",
    "        lines = soup_article.find('div', class_='tbody m-tcol-c').stripped_strings\n",
    "        content = ' '.join(lines)\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        content = content.replace(pattern2, '')\n",
    "        content = content.replace('\\n', '')\n",
    "        content = content.replace('\\u200b', '')\n",
    "\n",
    "        retries -= 1  # 재시도 횟수 감소\n",
    "\n",
    "    keyword = keyword.replace('\"', '')\n",
    "    results.append({\n",
    "        'Keyword': keyword,\n",
    "        'Title': title,\n",
    "        'Date': date,\n",
    "        'Link': 'https://cafe.naver.com/directwedding' + link,\n",
    "        'Content': content\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data=results)\n",
    "df.to_csv(\"naver_cafe.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI모델 게시글 분리  \n",
    "일반모델은 general_df로, AI모델은 ai_df로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjlee\\AppData\\Local\\Temp\\ipykernel_10040\\2789329052.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ai_df['Keyword'] = 'AI ' + ai_df['Keyword']\n"
     ]
    }
   ],
   "source": [
    "general_df = df[~df['Content'].str.contains('AI', case=False, na=False)]\n",
    "ai_df = df[df['Content'].str.contains('AI', case=False, na=False)]\n",
    "ai_df['Keyword'] = 'AI ' + ai_df['Keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일반모델: 1842\n",
      "AI모델: 103\n",
      "총 개수: 1945\n",
      "\n",
      "무선스틱청소기\n",
      "6\n",
      "0\n",
      "6\n",
      "\n",
      "무선청소기\n",
      "892\n",
      "36\n",
      "928\n",
      "\n",
      "lg 무선청소기\n",
      "138\n",
      "3\n",
      "141\n",
      "\n",
      "삼성 무선청소기\n",
      "111\n",
      "13\n",
      "124\n",
      "\n",
      "다이슨 무선청소기\n",
      "32\n",
      "0\n",
      "32\n",
      "\n",
      "로봇청소기\n",
      "944\n",
      "67\n",
      "1011\n"
     ]
    }
   ],
   "source": [
    "print(\"일반모델:\", general_df.shape[0])\n",
    "print(\"AI모델:\", ai_df.shape[0])\n",
    "print(\"총 개수:\", df.shape[0])\n",
    "\n",
    "print(\"\\n무선스틱청소기\")\n",
    "print((general_df['Keyword'].str.contains('무선스틱청소기')).sum())\n",
    "print((ai_df['Keyword'].str.contains('무선스틱청소기')).sum())\n",
    "print((df['Keyword'].str.contains('무선스틱청소기')).sum())\n",
    "\n",
    "print(\"\\n무선청소기\")\n",
    "print((general_df['Keyword'].str.contains('무선청소기')).sum())\n",
    "print((ai_df['Keyword'].str.contains('무선청소기')).sum())\n",
    "print((df['Keyword'].str.contains('무선청소기')).sum())\n",
    "\n",
    "print(\"\\nlg 무선청소기\")\n",
    "print((general_df['Keyword'].str.contains('lg 무선청소기')).sum())\n",
    "print((ai_df['Keyword'].str.contains('lg 무선청소기')).sum())\n",
    "print((df['Keyword'].str.contains('lg 무선청소기')).sum())\n",
    "\n",
    "print(\"\\n삼성 무선청소기\")\n",
    "print((general_df['Keyword'].str.contains('삼성 무선청소기')).sum())\n",
    "print((ai_df['Keyword'].str.contains('삼성 무선청소기')).sum())\n",
    "print((df['Keyword'].str.contains('삼성 무선청소기')).sum())\n",
    "\n",
    "print(\"\\n다이슨 무선청소기\")\n",
    "print((general_df['Keyword'].str.contains('다이슨 무선청소기')).sum())\n",
    "print((ai_df['Keyword'].str.contains('다이슨 무선청소기')).sum())\n",
    "print((df['Keyword'].str.contains('다이슨 무선청소기')).sum())\n",
    "\n",
    "print(\"\\n로봇청소기\")\n",
    "print((general_df['Keyword'].str.contains('로봇청소기')).sum())\n",
    "print((ai_df['Keyword'].str.contains('로봇청소기')).sum())\n",
    "print((df['Keyword'].str.contains('로봇청소기')).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.to_csv(\"청소기.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "ai_df.to_csv(\"AI청소기.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
